<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tianhe Ren</title>
  
  <meta name="author" content="Tianhe Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianhe Ren</name>
              </p>
              <p> 
                I am currently working at <a href="https://idea.edu.cn/">The International Digital Economy Academy (IDEA)</a> as Computer Vision Engineer, advised by Prof. <a href="https://www.leizhang.org/"> Lei Zhang </a>. In 2021, I got my bachelor's degree from <a href="https://mac.xmu.edu.cn/">MAC Lab</a> in Xiamen University advised by
                associate professor <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao">Yiyi Zhou</a> and Prof. <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>.
              </p>
              <p>
              I am interested in computer vision and deep learning. My current research focuses on:
                <li style="margin: 5px;" >
                  <b>Instance-level recognition tasks</b> including Object Detection, Instance Segmentation, Panoptic Segmentation.
                </li>
                <li style="margin: 5px;" >
                  <b>General visual architectures and Multi-Modality tasks</b> including the general visual backbone design, vision-language pretraining
                  visual grounding, etc.
                </li>
                <li style="margin: 5px;" >
                  <b>Deep generative models</b> including GAN, Diffusion, etc.
                </li>
                <li style="margin: 5px;" >
                  <b>Large language models</b>
                </li>
              </p>
              <p style="text-align:center">
                <a href="mailto:rentianhe@idea.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/rentainhe"> Github </a>&nbsp/&nbsp
                <a href="https://www.zhihu.com/people/shi-zhi-tou-xi-de-yang-guang">ZhiHu</a> 
              </p>
            </td>
            <!-- <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/rym.jpg">
            </td> -->
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2023-05:</b> <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-SAM</a> has been accepted to present a <strong>demo</strong> at <a href="https://iccv2023.thecvf.com/">ICCV 2023</a> in Paris.
              </li>
              <li style="margin: 5px;" >
                <b>2023-04:</b> Preprint <a href="https://arxiv.org/abs/2305.15023">LaVIN</a>, the first end-to-end efficient large vision-language instructed model.
              </li>
              <li style="margin: 5px;" >
                <b>2023-04:</b> Preprint <a href="https://github.com/IDEA-Research/Stable-DINO">Stable-DINO</a> and <a href="https://arxiv.org/abs/2304.13027">Focal-Stable-DINO</a>, achieving <b>64.8AP</b> on COCO test-dev <b>without test time augmentation</b> using <a href="https://github.com/microsoft/FocalNet">FocalNet-Huge</a> backbone (about 700M parameters). <font color="#FF0000">Rank Top-2 on COCO Leaderboard (2023.04)</font>
              </li>
              <li style="margin: 5px;" >
                <b>2023-04:</b> Release <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Grounded-SAM</a>, which aims to <b>detect and segment anything with text inputs</b>. <font color="#FF0000">Github Trending Top-1 Project (2023.04)</font>
              </li>
              <li style="margin: 5px;" >
                <b>2023-02:</b> <a href="https://github.com/hujiecpp/YOSO">YOSO</a> is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-10:</b> Release <a href="https://github.com/IDEA-Research/detrex">detrex</a> for benchmarking <b>DETR-based</b> algorithms.
              </li>
              <li style="margin: 5px;" >
                <b>2022-09:</b> <a href="https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization"> SparseSAM </a></a>is accepted to <a href="https://neurips.cc/Conferences/2022">NeurIPS 2022</a>.
              </li>
              <!-- <li style="margin: 5px;" >
                <b>2022-03:</b> Release the large-scale parallel training toolbox <a href="https://github.com/Oneflow-Inc/libai">LiBai</a> which supports 
                <b>Data/Model/Pipeline Parallelism Training</b> on a range of CV and NLP models.
              </li> -->
              <!-- <li style="margin: 5px;" >
                <b>2021-07:</b> <a href="https://github.com/rentainhe/TRAR-VQA"> TRAR </a></a>is accepted to <a href="https://iccv2021.thecvf.com/home">ICCV 2021</a>.
              </li> -->
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                See full list at <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao">Google Scholar.</a> (* indicates equal contribution)
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/LaVIN.gif" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</papertitle>
              </a>
              <br>
              <a href="https://luogen1996.github.io/"> Gen Luo</a>,
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
              <strong>Tianhe Ren</strong>,
              <a href=""> Shengxin Chen </a>,
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun </a>, 
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji </a>
              <br>
              <em>ArXiv, 2023</em>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2305.15023">paper</a> /
                <a href="https://github.com/luogen1996/LaVIN">code</a>
              </div>
              <p>A novel end-to-end efficient large vision-language instructed model.</p>
              <!-- <a href="https://arxiv.org/pdf/2305.15023.pdf">[arXiv]</a>
              <a href="https://github.com/luogen1996/LaVIN">[Code]</a> -->
              <!-- <a href="https://zhuanlan.zhihu.com/p/422838496">[中文解读]</a> -->
              <!-- <img alt="Github stars" src="https://github.com/luogen1996/LaVIN?style=social"> -->
              <!-- <p> The first end-to-end efficient large vision-language instructed model.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Focal-Stable-DINO.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2304.13027">
              <papertitle>A Strong and Reproducible Object Detector
                with Only Public Datasets</papertitle>
              </a>
              <br>
              <strong>Tianhe Ren*</strong>, 
              <a href="https://jwyang.github.io/"> Jianwei Yang*</a>,
              <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
              <a href="https://ailingzeng.site/"> Ailing Zeng*</a>,
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>,
              <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"> Hao Zhang</a>,
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang</a>
              <br>
              <em>ArXiv, 2023</em>
              <br>
              <font color="#FF0000">The Strongest Detector under 1B Parameters. Rank Top-2 on COCO Leaderboard (2023.04)</font>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.13027">paper</a> /
                <a href="https://github.com/IDEA-Research/Stable-DINO">code</a>
              </div>
              <p>A simple and strong detector achieves 64.8 AP on COCO test-dev without test time augmentation.</p>
              <!-- <a href="https://arxiv.org/abs/2304.13027">[arXiv]</a>
              <a href="https://github.com/microsoft/FocalNet">[FocalNet]</a>
              <a href="https://github.com/IDEA-Research/Stable-DINO">[Stable-DINO]</a>
              <img alt="Github stars" src="https://img.shields.io/github/stars/microsoft/FocalNet?style=social"> -->
              <!-- <p> We proposed <b>Focal-Stable-DINO</b> by combining strong FocalNet backbone and effective Stable-DINO detector.
                Remarkably, <b>without any test time augmentation</b>, our model achieves <b>64.6 AP</b> on COCO val2017 and <b>64.8 AP</b> on COCO test-dev with only publicly avaliable datasets for training.
              </p> -->
              <!-- <p> Combine FocalNet-Huge and Stable-DINO, we achieved <b>64.6 AP</b> on COCO val2017 and <b>64.8 AP</b> on COCO test-dev <b>without any test time augmentation</b>. 
              </p> -->
              <!-- <p> <i> "Like a <a href="https://github.com/zzyunzhi/object-intrinsics">rose</a>, each person can unfold their own unique beauty. May we witness a flourishing of varied approaches and techniques in this field." </i> 
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/stable-dino-arch.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2304.04742">
              <papertitle>Detection Transformer with Stable Matching</papertitle>
              </a>
              <br>
              <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
              <strong>Tianhe Ren*</strong>, 
              <a href="https://github.com/tuofeilunhifi"> Jiayu Chen*</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"> Hao Zhang</a>,
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>,  
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
              <a href="https://github.com/IDEA-Research/Stable-DINO"> Jun Huang</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang</a>
              <br>
              <em>ArXiv, 2023</em>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.04742">paper</a> /
                <a href="https://github.com/IDEA-Research/Stable-DINO">code</a>
              </div>
              <p>A new loss design to address the unstable matching issue in DETR-based models.</p>
              <!-- <a href="https://arxiv.org/abs/2304.04742">[arXiv]</a>
              <a href="https://github.com/IDEA-Research/Stable-DINO">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/IDEA-Research/StableDINO?style=social"> -->
              <!-- <p> We propose two simple yet effective
                modifications by integrating positional metrics to DETR’s
                classification loss and matching cost, named positionsupervised loss and position-modulated cost to solve the unstable
                matching problem. And our Stable-DINO achieves <b>63.8AP</b> on COCO test-dev with a Swin-Large backbone.
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GroundingDINO.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2303.05499">
              <papertitle>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</papertitle>
              </a>
              <br>
              <a href="http://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>, 
              <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"> Hao Zhang</a>, 
              <a href="https://github.com/yangjie-cv"> Jie Yang</a>,
              <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&hl=zh-CN&oi=ao"> Chunyuan Li</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang</a>
              <br>
              <em>ArXiv, 2023</em>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2303.05499">paper</a> /
                <a href="https://github.com/IDEA-Research/GroundingDINO">code</a>
              </div>
              <p>A simple and strong framework for open-set detection.</p>
              <!-- <a href="https://arxiv.org/abs/2303.05499">[arXiv]</a>
              <a href="https://github.com/IDEA-Research/GroundingDINO">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/IDEA-Research/GroundingDINO?style=social"> -->
              <!-- <p> GroundingDINO is an open-set object detector which can detect arbitrary objects with human inputs. GroundingDINO set a new record on COCO,
                LVIS, ODinW and RefCOCO/+/g. GroundingDINO achieves <b>zero-shot 52.5 AP</b> on COCO val2017 (training without COCO data) and <b>63.0 AP</b> (finetuned on COCO).
              </p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CVPR23_YOSO.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2303.14651">
              <papertitle>You Only Segment Once: Towards Real-Time Panoptic Segmentation</papertitle>
              </a>
              <br>
              <a href="https://hujiecpp.github.io/"> Jie Hu</a>,
              <a> Linyan Huang</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN"> Rongrong Ji </a>, 
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao </a>,
              <br>
              <em>CVPR, 2023</em>
              <!-- <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>) </em>, 2023 -->
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2303.14651">arxiv</a> /
                <a href="https://github.com/hujiecpp/YOSO">code</a>
              </div>
              <p>A novel framework for real-time panoptic segmentation task with competitive performance compared to state-of-the-art methods.</p>
              <!-- <a href="https://arxiv.org/abs/2303.14651">[arXiv]</a>
              <a href="https://github.com/hujiecpp/YOSO">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/hujiecpp/YOSO?style=social"> -->
              <!-- <p> YOSO designed a <b>light-weight feature pyramid aggregator</b> and a <b>seperable dynamic convolution attention module</b> to speed up the segmentation pipeline.
                To the best of our knowledge, YOSO is <b>the first real-time panoptic segmentation framework</b> with competitive performance compared to state-of-the-art methods.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/iu_vit.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2212.13771">
              <papertitle>Exploring Vision Transformers as Diffusion Learners</papertitle>
              </a>
              <br>
              <a href="https://github.com/CiaoHe"> He Cao</a>,
              <a href="https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=zh-CN&oi=ao">Jianan Wang</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=odjSydQAAAAJ&hl=zh-CN&oi=ao"> Xianbiao Qi</a>, 
              <a href="https://scholar.google.com/citations?user=IA_3Jq8AAAAJ&hl=zh-CN&oi=ao"> Yihao Chen </a>, 
              <a href="https://yao-lab.github.io/"> Yuan Yao </a>,
              <a href="https://www.leizhang.org/"> Lei Zhang </a>
              <br>
              <em>ArXiv, 2022</em>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2212.13771">arxiv</a>
              </div>
              <p>A plain, non-hierarchical Vision Transformer (ViT) backbone for diffusion models.</p>
              <!-- <a href="https://arxiv.org/abs/2212.13771">[arXiv]</a>
              <a href="">[Code Coming Soon]</a> -->
              <!-- <a href="https://hornet.ivg-research.xyz/">[Project Page]</a> 
              <a href="https://mp.weixin.qq.com/s/MyMIPv-bn9wVMLABurjOUA">[中文解读]</a>  -->
              <!-- <p> We are using plain vit structure as diffusion backbone and propose <b>IU-ViT</b> and <b>ASCEND</b> which close the performance gap between ViT and U-Net and we're the first work to explore one-stage large resolution text to image generation.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SparseSAM.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://arxiv.org/abs/2210.05177">
              <papertitle>Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=PTM4HCsAAAAJ&hl=zh-CN&oi=ao"> Peng Mi</a>,
              <a href="https://sites.google.com/site/mathshenli/home">Li Shen</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>, 
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun </a>, 
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji </a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html"> Dacheng Tao </a>
              <br>
              <em>NIPS, 2022</em>
              <!-- <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022 -->
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2210.05177">arxiv</a> /
                <a href="https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization">code</a>
              </div>
              <p>An efficient variant of SAM optimizer achieved by computing a sparse perturbation based on fisher information and dynamic sparse training.</p>
              <!-- <a href="https://arxiv.org/abs/2210.05177">[arXiv]</a>
              <a href="https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/Mi-Peng/Sparse-Sharpness-Aware-Minimization?style=social"> -->
              <!-- <p> SparseSAM is an efficient variant of SAM optimizer achieved by computing a <b>sparse perturbation</b> based on fisher information and dynamic sparse training.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/TRAR.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">
              <papertitle>TRAR: Routing the Attention Spans in Transformers for Visual Question Answering</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=jkxdiToAAAAJ&hl=zh-CN&oi=ao"> Chaoyang Zhu </a>, 
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun </a>, 
              <a href="https://people.ucas.ac.cn/~jzliu?language=en"> Jianzhuang Liu </a>,
              <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN"> Xinghao Ding </a>,
              <a href="https://scholar.google.com.hk/citations?user=u-8x34cAAAAJ&hl=zh-CN"> Mingliang Xu </a>,
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji </a>
              <br>
              <em>ICCV, 2021</em>
              <!-- <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021 -->
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">paper</a> /
                <a href="https://github.com/rentainhe/TRAR-VQA">code</a>
              </div>
              <!-- <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">[Paper]</a>
              <a href="https://github.com/rentainhe/TRAR-VQA">[Code]</a>
              <a href="https://zhuanlan.zhihu.com/p/422838496">[中文解读]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/rentainhe/TRAR-VQA?style=social"> -->
              <p>A novel dynamic routing attention machinism for vision and language tasks.</p>
              <!-- <p> TRAR design a <b>dynamic routing attention</b> mechanism for vision and language tasks that achieves new SOTA on VQA2.0, CLEVR and REC. </p> -->
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <p><heading>Open Source Projects</heading></p>
            <p>
               * indicates project lead, # indicates directional lead
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/Grounded_SAM.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">
            <papertitle>Grounded-SAM: Detect, Segment and Generate Anything</papertitle>
            </a>
            <br>
            <strong>Tianhe Ren*</strong>,
            <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
            <a href="https://github.com/CiaoHe"> He Cao*</a>, 
            <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li*</a>, 
            <a href="https://scholar.google.com/citations?user=B8hPxMQAAAAJ&hl=zh-CN"> Hao Zhang*</a>, 
            <a href="https://scholar.google.com/citations?user=D4tLSbsAAAAJ&hl=zh-CN"> Kunchang Li</a>, 
            <a href="https://github.com/tuofeilunhifi"> Jiayu Chen </a>, 
            <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
            <a href="https://www.leizhang.org/"> Lei Zhang#</a>
            <br>
            <font color="#FF0000">GitHub Trending Top-1 Project (2023.04)</font>
            <br>
            <em>ICCV Demo Track, 2023</em>
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">project</a> /
              <a href="https://zhuanlan.zhihu.com/p/620271321">introduction (in Chinese)</a>
            </div>
            <p> <b>Grounded-SAM</b> is a general application aims to combine SoTA zero-shot detect, segment and generative models for automatic label and generate data system.
            </p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/detr_arch.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <a href="https://github.com/IDEA-Research/detrex">
            <papertitle>detrex: research platform for transformer-based instance recognition algorithms.</papertitle>
            </a>
            <br>
            <strong>Tianhe Ren*</strong>,
            <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
            <a href="https://scholar.google.com.hk/citations?user=B8hPxMQAAAAJ&hl=zh-CN&oi=sra"> Hao Zhang*</a>, 
            <a href="https://fengli-ust.github.io/"> Feng Li*</a>, 
            <a href="https://github.com/L1aoXingyu"> Xingyu Liao </a>, 
            <a href="https://www.leizhang.org/"> Lei Zhang# </a>
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://github.com/IDEA-Research/detrex">project</a> /
              <a href="https://detrex.readthedocs.io/en/latest/">documentation</a> /
              <a href="https://zhuanlan.zhihu.com/p/571307593">introduction (in Chinese)</a>
            </div>
            <!-- <a href="https://github.com/IDEA-Research/detrex">[Project Page]</a>
            <a href="https://detrex.readthedocs.io/en/latest/">[Documentation]</a>
            <a href="https://zhuanlan.zhihu.com/p/571307593">[中文介绍]</a>
            <br> -->
            <p> <b>detrex</b> is a research platform that supports a series of transformer-based object detection, semantic/instance/panoptic segmentation, pose estimation algorithms.</p>
            <!-- <p> <b>detrex</b> is a research platform that supports a series of transformer-based object detection algorithms including <b>DETR (ECCV 2020)</b>, <b>Deformable-DETR (ICLR 2021)</b>
              <b>Conditional-DETR (ICCV 2021)</b>, <b>DAB-DETR (ICLR 2022)</b>, <b>DN-DETR (CVPR 2022)</b>, <b>DINO (ArXiv 2022)</b>, <b>MaskDINO (ArXiv 2022)</b> etc.</p> -->
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/libai.png" alt="dise">
          </td>
          <td width="75%" valign="center">
            <a href="https://github.com/Oneflow-Inc/libai">
            <papertitle>LiBai(李白): A Toolbox for Large-Scale Distributed Parallel Training.</papertitle>
            </a>
            <br>
            <a href="https://github.com/L1aoXingyu"> Xingyu Liao*#</a>,
            <a href="https://github.com/CPFLAME"> Peng Cheng*</a>,
            <strong>Tianhe Ren*</strong>,
            <a href="https://github.com/Ldpe2G"> Depeng Liang</a>,
            <a href="https://github.com/dangkai4u"> Kai Dang</a>, 
            <a href="https://github.com/marigoold"> Yi Wang</a>, 
            <a href="https://github.com/strint"> Xiaoyu Xu </a>
            <br>
            <!-- <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022 -->
            <!-- <br> -->
            <div class="paper" id="boundary_iou">
              <a href="https://github.com/Oneflow-Inc/libai">project</a> /
              <a href="https://libai.readthedocs.io/en/latest/">documentation</a> /
              <a href="https://zhuanlan.zhihu.com/p/575075055">introduction (in Chinese)</a>
            </div>
            <p> <b>LiBai</b> is a is an open source large-scale model training toolbox based on OneFlow.</p>
            <!-- <a href="https://github.com/Oneflow-Inc/libai">[Project Page]</a>
            <a href="https://libai.readthedocs.io/en/latest/">[Documentation]</a>
            <a href="https://zhuanlan.zhihu.com/p/575075055">[中文介绍]</a> 
            <img alt="Github stars" src="https://img.shields.io/github/stars/Oneflow-inc/libai?style=social"> -->
            <!-- <p> <b>LiBai</b> is a is an open source large-scale model training toolbox based on OneFlow that supports <b>Data Parallelism/ Model Parallelism/ Pipeline Parallelism Training</b> 
              on a range of CV and NLP models including <b>Vision Transformer</b>, <b>Swin-Transformer</b>, <b>BERT</b>, <b>T5</b>, <b>GPT-2</b>, etc.</p> -->
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <img style="width:100%;max-width:100%" src="images/SimREC.jpeg" alt="dise">
          </td>
          <td width="75%" valign="center">
            <a href="https://github.com/luogen1996/SimREC">
            <papertitle>SimREC: a lightweight codebase for referring expression comprehension and segmentation.</papertitle>
            </a>
            <br>
            <a href="https://luogen1996.github.io/"> Gen Luo*#</a>,
            <strong>Tianhe Ren*</strong>
            <br>
            <!-- <em>Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2022 -->
            <div class="paper" id="boundary_iou">
              <a href="https://github.com/luogen1996/SimREC">project</a> /
              <a href="https://simrec.readthedocs.io/en/latest/">documentation</a>
            </div>
            <!-- <a href="https://github.com/luogen1996/SimREC">[Project Page]</a>
            <a href="https://simrec.readthedocs.io/en/latest/">[Documentation]</a>
            <img alt="Github stars" src="https://img.shields.io/github/stars/luogen1996/SimREC?style=social">
            <br> -->
            <p> <b>SimREC</b> is a simple and lightweight codebase for the research of referring expression comprehension and segmentation, 
              supporting large-scale pre-training and multi-task learning.</p>
          </td>
        </tr>

      </tbody></table>

          <!-- <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script> -->

      <!-- <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr></tbody></table>
      </div> -->


        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> 1st place in the MVP Point Cloud Completion Challenge (ICCV 2021 Workshop)</li>
                <li style="margin: 5px;"> Baidu Top 100 Chinese Rising Stars in AI <a href="https://mp.weixin.qq.com/s/v7ITiZXOJiDUbPlRlcqQRA">(百度AI华人新星百强榜)</a></li>
                <li style="margin: 5px;"> CVPR 2021 Outstanding Reviewer</li>
                <li style="margin: 5px;"> ECCV 2020 Outstanding Reviewer</li>
                <li style="margin: 5px;"> 2nd place in Semi-Supervised Recognition Challenge at FGVC7 (CVPR 2020 Workshop)</li>
                <li style="margin: 5px;"> 2019 CCF-CV Academic Emerging Award (CCF-CV 学术新锐奖)</li>
                <li style="margin: 5px;"> 2019 Chinese National Scholarship </li>
                <li style="margin: 5px;"> ICME 2019 Best Reviewer Award </li>
                <li style="margin: 5px;"> 2017 Sensetime Undergraduate Scholarship </li>
                <li style="margin: 5px;"> 1st place in 17th Electronic Design Contest of Tsinghua University </li>
                <li style="margin: 5px;"> 1st place in Momenta Lane Detection Challenge </li>
              </p>
            </td>
          </tr>
        </tbody></table> -->

        <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Co-organizer:</b> Tutorial on Deep Reinforcement Learning for Computer Vision at CVPR 2019 <a href="http://ivg.au.tsinghua.edu.cn/DRLCV/"> [website]</a>
              </li>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> CVPR 2018-2022, ICCV 2019-2021, ECCV 2020-2022, NeurIPS 2019-2022, ICML 2019-2022, ICLR 2021-2023, SIGGRAPH Asia 2022, AAAI 2020-2022, WACV 2020-2022, ICME 2019-2022, 
              </li>
              <li style="margin: 5px;"> 
                <b>Senior PC Member:</b> IJCAI 2021
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> T-PAMI, IJCV, T-NNLS, T-IP, T-MM, Pattern Recognition
              </li>
            </p>
          </td>
        </tr>
      </tbody></table> -->
       
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<!-- <p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
	  </div>        
	  <br>
	    &copy; Tianhe Ren | Last updated: Nov 9, 2022
</center></p>
</body> -->

</html>
