<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  
  .research-image-container {
    overflow: hidden;
    border-radius: 10px;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    transition: all 0.3s ease;
    width: 100%;
    height: 170px; /* 固定高度 */
    display: flex;
    align-items: center;
    justify-content: center;
    position: relative;
    cursor: pointer; /* 添加指针样式提示可点击 */
  }
  
  .research-image-container:hover {
    transform: translateY(-5px);
    box-shadow: 0 8px 20px rgba(41, 98, 255, 0.2);
  }
  
  .research-image {
    width: 100%;
    height: 100%;
    display: block;
    border-radius: 10px;
    transition: all 0.5s ease;
    object-fit: cover; /* 确保图片填充容器并保持比例 */
  }
  
  .research-image-container:hover .research-image {
    position: absolute;
    z-index: 100;
    object-fit: contain; /* 悬停时显示完整图片 */
    background-color: white;
    box-shadow: 0 10px 25px rgba(0, 0, 0, 0.2);
    height: auto;
    max-height: 300px; /* 可以根据需要调整 */
    transform: scale(1.1);
  }
  
  /* 模态框样式 */
  .modal {
    display: none;
    position: fixed;
    z-index: 1000;
    padding-top: 50px;
    left: 0;
    top: 0;
    width: 100%;
    height: 100%;
    overflow: auto;
    background-color: rgba(0, 0, 0, 0.9);
  }
  
  .modal-content {
    margin: auto;
    display: block;
    max-width: 80%;
    max-height: 80%;
    object-fit: contain;
  }
  
  .close-modal {
    position: absolute;
    top: 20px;
    right: 35px;
    color: #f1f1f1;
    font-size: 40px;
    font-weight: bold;
    transition: 0.3s;
    cursor: pointer;
  }
  
  .close-modal:hover {
    color: #2962ff;
  }
  
  /* 论文悬停效果 */
  .paper-item {
    transition: all 0.3s ease;
    border-radius: 8px;
  }
  
  .paper-item:hover {
    background-color: rgba(41, 98, 255, 0.05);
    transform: translateY(-5px);
    box-shadow: 0 10px 20px rgba(0, 0, 0, 0.08);
  }
  
  .paper-item:hover .research-image-container {
    box-shadow: 0 8px 25px rgba(41, 98, 255, 0.25);
  }
  
  .paper-item:hover papertitle {
    color: #2962ff;
  }
</style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <title>Tianhe Ren</title>
  
  <meta name="author" content="Tianhe Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
</head>


<body>
  <table id="mainContent" style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianhe Ren</name>
              </p>
              <p style="line-height:1.7;margin-bottom:20px;text-align:justify;color:#333;"> 
                I am currently working at <a href="https://idea.edu.cn/" style="color:#2962ff;text-decoration:none;font-weight:500;border-bottom:1px dashed #2962ff;transition:all 0.3s">The International Digital Economy Academy (IDEA)</a> as Computer Vision Engineer, advised by Prof. <a href="https://www.leizhang.org/" style="color:#2962ff;text-decoration:none;font-weight:500;border-bottom:1px dashed #2962ff;transition:all 0.3s">Lei Zhang</a>. 
                
                <span style="display:block;margin-top:10px;">In 2021, I got my bachelor's degree from <a href="https://mac.xmu.edu.cn/" style="color:#2962ff;text-decoration:none;font-weight:500;border-bottom:1px dashed #2962ff;transition:all 0.3s">MAC Lab</a> in Xiamen University advised by Associate Prof. <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao" style="color:#2962ff;text-decoration:none;font-weight:500;border-bottom:1px dashed #2962ff;transition:all 0.3s">Yiyi Zhou</a> and Prof. <a href="https://mac.xmu.edu.cn/rrji/" style="color:#2962ff;text-decoration:none;font-weight:500;border-bottom:1px dashed #2962ff;transition:all 0.3s">Rongrong Ji</a>.</span>
              </p>
              
              <div style="background:#f8f9fa;padding:15px;border-radius:8px;margin:20px 0;border-left:4px solid #2962ff;">
                <p style="margin:0;line-height:1.7;color:#333;">
                  I'm primarily interested in researching <strong style="color:#2962ff">vision foundation models</strong>, <strong style="color:#2962ff">object detection and segmentation</strong>, and <strong style="color:#2962ff">multi-modal learning</strong>. I'm also passionate about open-source projects in AI community. The research work and open-source projects I'm involved in have garnered <strong style="color:#2962ff">more than 30.0K stars</strong> on Github.
                </p>
              </div>
              
              <p style="font-style:italic;color:#666;text-align:center;font-size:0.9em;margin-top:15px;">
                <span style="display:inline-block;">✨ My personal avatar is my beloved daughter named Toffee, she is a Minuet (also known as the Napoleon) cat ✨</span>
              </p>
              
              <div style="text-align:center;margin-top:25px;">
                <a href="mailto:rentianhe@idea.edu.cn" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="far fa-envelope" style="margin-right:5px;"></i> Email
                </a>
                
                <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="fas fa-graduation-cap" style="margin-right:5px;"></i> Google Scholar
                </a>
                
                <a href="./files/Ren_Tianhe_CV-1201.pdf" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="far fa-file-alt" style="margin-right:5px;"></i> Resume
                </a>
                
                <a href="https://github.com/rentainhe" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="fab fa-github" style="margin-right:5px;"></i> Github
                </a>
                
                <a href="https://x.com/Tianhe_Ren" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="fab fa-twitter" style="margin-right:5px;"></i> Twitter
                </a>
                
                <a href="https://www.zhihu.com/people/shi-zhi-tou-xi-de-yang-guang" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="fab fa-zhihu" style="margin-right:5px;"></i> ZhiHu
                </a>

                <!-- 修改Blogs按钮链接到blogs.html -->
                <a href="blogs.html" style="display:inline-block;margin:5px 8px;padding:8px 15px;background:#f5f7fa;color:#2962ff;text-decoration:none;border-radius:20px;font-weight:500;transition:all 0.3s;box-shadow:0 2px 5px rgba(0,0,0,0.05);" onmouseover="this.style.background='#2962ff';this.style.color='white';" onmouseout="this.style.background='#f5f7fa';this.style.color='#2962ff';">
                  <i class="fas fa-blog" style="margin-right:5px;"></i> Blogs (under test)
                </a>
              </div>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%;vertical-align:middle">
              <div style="position:relative;display:inline-block;border-radius:15px;overflow:hidden;box-shadow:0 10px 30px rgba(0,0,0,0.1);transition:all 0.5s;" onmouseover="this.style.transform='scale(1.03)';this.style.boxShadow='0 15px 40px rgba(41,98,255,0.15)';" onmouseout="this.style.transform='scale(1)';this.style.boxShadow='0 10px 30px rgba(0,0,0,0.1)';">
                <img style="width:100%;max-width:100%;display:block;" alt="Toffee the cat" src="images/toffee.png">
                <div style="position:absolute;bottom:0;left:0;right:0;padding:10px;background:linear-gradient(to top, rgba(0,0,0,0.7), transparent);text-align:center;">
                  <span style="color:white;font-size:0.9em;">Toffee</span>
                </div>
              </div>
            </td>
          </tr>
        </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <p><heading>News</heading></p>
                  <div class="news-container" style="max-height:400px;overflow-y:auto;padding-right:15px">
                    <!-- 2025 -->
                    <div style="margin:20px 0">
                      <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px">2025</h3>
                      <ul style="list-style:none;padding:0">
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">We have released the <a href="https://deepdataspace.com/blog/dino-xseek" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">DINO-XSeeK</a> model, aimed at more accurately detecting objects based more complex text descriptions.</p>
                        </li>
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">One paper is accepted to TPAMI 2025.</p>
                        </li>
                      </ul>
                    </div>

                    <!-- 2024 -->
                    <div style="margin:20px 0">
                      <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px">2024</h3>
                      <ul style="list-style:none;padding:0">
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">We have released <a href="https://arxiv.org/abs/2411.14347" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">DINO-X</a>, the world's top-performing vision model for open-world detection and understanding.</p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="https://arxiv.org/abs/2303.05499" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounding DINO</a> is selected as 
                            <a href="https://www.paperdigest.org/2024/10/most-influential-arxiv-computer-vision-and-pattern-recognition-papers-2024-10/" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">The 10th Most Influential Paper</a> among the 2023 Computer Vision and Pattern Recognition ArXiv papers by PaperDigest.
                          </p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="https://arxiv.org/abs/2401.14159" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounded SAM</a> is selected as 
                            <a href="https://www.paperdigest.org/2024/10/most-influential-arxiv-computer-vision-and-pattern-recognition-papers-2024-10/" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">The 7th Most Influential Paper</a> among the 2024 Computer Vision and Pattern Recognition ArXiv papers by PaperDigest.
                          </p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="https://arxiv.org/abs/2303.05499" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounding DINO</a> is selected as 
                            <a href="https://www.paperdigest.org/2024/09/most-influential-eccv-papers-2024-09/" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">The 1st Most Influential Paper</a> in ECCV 2024 by PaperDigest.
                          </p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounded SAM 2</a> is released towards tracking anything in video.
                          </p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">One paper is accepted to NeurIPS 2024.</p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="https://www.deepdataspace.com/blog/Grounding-DINO-1.6-Pro" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounding DINO 1.6</a> is released with stronger open-world detection performance.
                          </p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">One paper is accepted to TPAMI 2024.</p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">Six papers are accepted to ECCV 2024.</p>
                        </li>

                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">We have released <a href="https://arxiv.org/abs/2405.10300" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounding DINO 1.5</a>, the most capable open-set detection model to date.</p>
                        </li>
                      </ul>
                    </div>

                    <!-- 2023 -->
                    <div style="margin:20px 0">
                      <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px">2023</h3>
                      <ul style="list-style:none;padding:0">
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">
                            <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything" style="color:#2962ff;text-decoration:none;border-bottom:1px dashed #2962ff">Grounded SAM</a> is accepted to ICCV 2023 Demo Track.
                          </p>
                        </li>
                        
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">Four papers are accepted to CVPR/NeurIPS/ICCV 2023.</p>
                        </li>
                      </ul>
                    </div>

                    <!-- 2022 -->
                    <div style="margin:20px 0">
                      <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px">2022</h3>
                      <ul style="list-style:none;padding:0">
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">One paper is accepted to NeurIPS 2022.</p>
                        </li>
                      </ul>
                    </div>

                    <!-- 2021 -->
                    <div style="margin:20px 0">
                      <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px">2021</h3>
                      <ul style="list-style:none;padding:0">
                        <li style="margin:15px 0;padding:12px 15px;background:linear-gradient(to right,#f8f9fa,white);border-left:4px solid #2962ff;border-radius:0 8px 8px 0;transition:all 0.3s">
                          <p style="margin:5px 0">One paper is accepted to ICCV 2021.</p>
                        </li>
                      </ul>
                    </div>

                  </div>
                </td>
              </tr>
            </tbody>
          </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Research</heading></p>
              <p>
                See full list at <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao">Google Scholar.</a> (* indicates equal contribution, † indicates corresponding author).
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr class="paper-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="research-image-container">
                <img src='images/DINO-X.jpg' alt="DINO-X" class="research-image">
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:top" >
              <a href="https://arxiv.org/abs/2411.14347">
              <papertitle>DINO-X: A Unified Vision Model for Open-World Object Detection and Understanding</papertitle>
              </a>
              <br>
              <strong>IDEA Research Team</strong>
              <br>
              Tech report, Nov, 2024. 
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2411.14347">Paper</a> /
                <a href="https://github.com/IDEA-Research/DINO-X-API">Code</a> /
                <a href="https://deepdataspace.com/home">Website</a> /
                <a href="https://deepdataspace.com/blog/dino-xseek">DINO-XSeeK (preview)</a>
              </div>
            </td>
          </tr>

          <tr class="paper-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="research-image-container">
                <img src='images/GroundingDINO.png' alt="Grounding DINO" class="research-image">
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:top" >
              <a href="https://arxiv.org/abs/2303.05499">
              <papertitle>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</papertitle>
              </a>
              <br>
              <a href="http://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>, 
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>, 
              <a href="https://github.com/yangjie-cv"> Jie Yang</a>,
              <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&hl=zh-CN&oi=ao"> Chunyuan Li</a>,
              <a href="https://jwyang.github.io/"> Jianwei Yang</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang†</a>
              <br>
              European Conference on Computer Vision (<strong>ECCV</strong>), 2024. 
              <br>
              <font color="red"><strong>The 1st Most Influential Paper</strong></font> by <a href="https://www.paperdigest.org/2024/09/most-influential-eccv-papers-2024-09/">PaperDigest</a>.
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2303.05499">Paper</a> /
                <a href="https://github.com/IDEA-Research/GroundingDINO">Code</a> /
                <a href="https://deepdataspace.com/home">Website</a> /
                <a href="https://www.youtube.com/watch?v=wxWDt5UiwY8">Video</a>
              </div>
            </td>
          </tr>

          <tr class="paper-item">
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <div class="research-image-container">
                <img src="images/t_rex2.jpg" alt="game" width="180" height="130" class="research-image">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2311.13596">
              <papertitle>T-Rex2: Towards Generic Object Detection via Text-Visual Prompt Synergy</papertitle>
              </a>
              <br>
              <a href="https://mountchicken.github.io/"> Qing Jiang</a>,
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ"> Zhaoyang Zeng</a>,
              <strong>Tianhe Ren</strong>,
              <a href="https://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang†</a>
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              European Conference on Computer Vision (<strong>ECCV</strong>), 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2311.13596">Paper</a> /
                <a href="https://github.com/IDEA-Research/T-Rex">Code</a> /
                <a href="https://llava-vl.github.io/llava-plus/">Website</a> /
                <a href="https://deepdataspace.com/playground/ivp">Demo</a> /
                <a href="https://www.youtube.com/watch?v=engIEhZogAQ">Video</a>
              </div>
            </td>
          </tr>

          <tr class="paper-item">
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <div class="research-image-container">
                <img src="images/gd1.5_framework.png" alt="game" width="180" height="130" class="research-image">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:top">
              <a href="https://arxiv.org/abs/2405.10300">
              <papertitle>Grounding DINO 1.5: Advance the "Edge" of Open-Set Object Detection</papertitle>
              </a>
              <br>
              <strong> Tianhe Ren*</strong>,
              <a href="https://mountchicken.github.io/"> Qing Jiang*</a>,
              <a href="http://lsl.zone/"> Shilong Liu*</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN"> Zhaoyang Zeng*</a>,
              <a href=""> Wenlong Liu</a>,
              <a href=""> Han Gao</a>,
              <a href=""> Hongjie Huang</a>,
              <a href=""> Zhengyu Ma</a>,
              <a href="https://scholar.google.com/citations?user=aDf9fpkAAAAJ&hl=zh-CN"> Xiaoke Jiang</a>,
              <a href="https://scholar.google.com/citations?user=IA_3Jq8AAAAJ&hl=zh-CN"> Yihao Chen</a>,
              <a href=""> Yuda Xiong</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <a href=""> Peijun Tang</a>,
              <a href=""> Kent Yu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang†</a>
              <br>
              Tech report, May. 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2405.10300">Paper</a> /
                <a href="https://github.com/IDEA-Research/Grounding-DINO-1.5-API">Code</a> /
                <a href="https://deepdataspace.com/home">Website</a> /
                <a href="https://huggingface.co/spaces/Mountchicken/Grounding-DINO-1.5-API">Demo</a> /
                <a href="https://www.deepdataspace.com/blog/Grounding-DINO-1.6-Pro">Grounding DINO 1.6 Tech Blog</a>
              </div>
            </td>
          </tr>

        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/grounded_sam_paper.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2401.14159">
            <papertitle>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</papertitle>
            </a>
            <br>
            <strong>Tianhe Ren*</strong>,
            <a href="http://lsl.zone/"> Shilong Liu*</a>,
            <a href="https://ailingzeng.site/"> Ailing Zeng</a>,
            <a href="https://jinglin7.github.io/"> Jing Lin</a>,
            <a href="https://andy1621.github.io/"> Kunchang Li</a>,
            <a href="https://scholar.google.com/citations?user=tLZ2V2kAAAAJ&hl=zh-CN"> He Cao</a>,
            <a href="https://github.com/tuofeilunhifi"> Jiayu Chen</a>,
            <a href="https://xinyu1205.github.io/"> Xinyu Huang</a>,
            <a href="https://yukangchen.com/"> Yukang Chen</a>,
            <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=en&oi=sra"> Feng Yan</a>,
            <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN"> Zhaoyang Zeng</a>,
            <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
            <a href="https://fengli-ust.github.io/"> Feng Li</a>,
            <a href="https://yangjie-cv.github.io/"> Jie Yang</a>,
            <a href="https://scholar.google.com/citations?user=zdgHNmkAAAAJ&hl=zh-CN"> Hongyang Li</a>,
            <a href="https://mountchicken.github.io/"> Qing Jiang</a>,
            <a href="https://www.leizhang.org/"> Lei Zhang†</a>
            <br>
            International Conference on Computer Vision (<strong>ICCV</strong>) Demo Track, 2023
            <br>
            <font color="red"><strong>The 7th Most Influential Paper</strong></font> in the 2024 Computer Vision and Pattern Recognition ArXiv papers by <a href="https://www.paperdigest.org/2024/10/most-influential-arxiv-computer-vision-and-pattern-recognition-papers-2024-10/">PaperDigest</a>.
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://arxiv.org/abs/2401.14159">Paper</a> /
              <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">Code</a> /
              <a href="https://www.youtube.com/watch?v=oEQYStnF2l8">Video</a> /
              <a href="https://github.com/IDEA-Research/Grounded-SAM-2">Grounded SAM 2</a>
            </div>
          </td>
        </tr>


        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/lavin.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2305.15023">
            <papertitle>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</papertitle>
            </a>
            <br>
            <a href="https://luogen1996.github.io/"> Gen Luo</a>,
            <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
            <strong>Tianhe Ren</strong>,
            <a href="">Shengxin Chen</a>,
            <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun</a>, 
            <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji†</a>
            <br>
            <!-- <em>NeurIPS</em>, 2023 -->
            Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://arxiv.org/abs/2305.15023">Paper</a> /
              <a href="https://github.com/luogen1996/LaVIN">Code</a> /
              <a href="https://luogen1996.github.io/lavin/">Website</a>
            </div>
          </td>
        </tr>

        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/stable_dino_new.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2304.04742">
            <papertitle>Detection Transformer with Stable Matching</papertitle>
            </a>
            <br>
            <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
            <strong>Tianhe Ren*</strong>, 
            <a href="https://github.com/tuofeilunhifi"> Jiayu Chen*</a>,
            <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
            <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
            <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>,  
            <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
            <a href="https://github.com/IDEA-Research/Stable-DINO"> Jun Huang</a>,
            <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
            <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
            <a href="https://www.leizhang.org/"> Lei Zhang†</a>
            <br>
            <!-- <em>ICCV</em>, 2023 -->
            International Conference on Computer Vision (<strong>ICCV</strong>), 2023
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://arxiv.org/abs/2304.04742">Paper</a> /
              <a href="https://github.com/IDEA-Research/Stable-DINO">Code</a>
            </div>
          </td>
        </tr>

        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/detrex.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top" >
            <a href="https://arxiv.org/abs/2306.07265">
            <papertitle>detrex: Benchmarking Detection Transformers</papertitle>
            </a>
            <br>
            <strong>Tianhe Ren*</strong>,
            <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
            <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li*</a>,
            <a href="https://haozhang534.github.io/"> Hao Zhang*</a>,
            <a href="https://ailingzeng.site/"> Ailing Zeng</a>,
            <a href="https://github.com/yangjie-cv"> Jie Yang</a>,
            <a href="https://github.com/L1aoXingyu"> Xingyu Liao</a>,
            <a href="https://github.com/JiaDingCN"> Ding Jia</a>,
            <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
            <a href="https://github.com/CiaoHe"> He Cao</a>,
            <a href="https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=en&oi=ao"> Jianan Wang</a>,
            <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN"> Zhaoyang Zeng</a>,
            <a href="https://scholar.google.com/citations?user=odjSydQAAAAJ&hl=zh-CN"> Xianbiao Qi</a>,
            <a href="https://scholar.google.com/citations?user=PzyvzksAAAAJ&hl=en"> Yuhui Yuan</a>,
            <a href="https://jwyang.github.io/"> Jianwei Yang</a>,
            <a href="https://www.leizhang.org/"> Lei Zhang†</a>
            <br>
            <!-- <em>Tech Report</em>, 2023 -->
            Tech report, May. 2023
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://arxiv.org/abs/2306.07265">Paper</a> /
              <a href="https://github.com/IDEA-Research/detrex">Code</a> /
              <a href="https://rentainhe.github.io/projects/detrex/">Website</a>
            </div>
          </td>
        </tr>

        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/CVPR23_YOSO.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://arxiv.org/abs/2303.14651">
            <papertitle>You Only Segment Once: Towards Real-Time Panoptic Segmentation</papertitle>
            </a>
            <br>
            <a href="https://hujiecpp.github.io/"> Jie Hu</a>,
            <a href=""> Linyan Huang</a>,
            <strong>Tianhe Ren</strong>, 
            <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
            <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN"> Rongrong Ji</a>, 
            <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao†</a>,
            <br>
            <!-- <em>CVPR</em>, 2023 -->
            Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://arxiv.org/abs/2303.14651">Paper</a> /
              <a href="https://github.com/hujiecpp/YOSO">Code</a>
            </div>
          </td>
        </tr>

        <tr class="paper-item">
          <td style="padding:20px;width:30%;max-width:30%" align="center">
            <div class="research-image-container">
              <img src="images/TRAR.png" alt="game" width="180" height="130" class="research-image">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:top">
            <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">
            <papertitle>TRAR: Routing the Attention Spans in Transformers for Visual Question Answering</papertitle>
            </a>
            <br>
            <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
            <strong>Tianhe Ren</strong>, 
            <a href="https://scholar.google.com.hk/citations?user=jkxdiToAAAAJ&hl=zh-CN&oi=ao"> Chaoyang Zhu </a>, 
            <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun</a>, 
            <a href="https://people.ucas.ac.cn/~jzliu?language=en"> Jianzhuang Liu</a>,
            <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN"> Xinghao Ding</a>,
            <a href="https://scholar.google.com.hk/citations?user=u-8x34cAAAAJ&hl=zh-CN"> Mingliang Xu</a>,
            <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji†</a>
            <br>
            International Conference on Computer Vision (<strong>ICCV</strong>), 2021
            <br>
            <div class="paper" id="boundary_iou">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">Paper</a> /
              <a href="https://github.com/rentainhe/TRAR-VQA">Code</a>
            </div>
          </td>
        </tr>

      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
          <heading>Work Experiences</heading>
          <!-- Conference Reviewer Section -->
          <div style="margin:20px 0;padding:20px;background:linear-gradient(to right,#f8f9fa,white);border-radius:10px;box-shadow:0 2px 5px rgba(0,0,0,0.05)">
            <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px;display:flex;align-items:center">
              <i class="fas fa-search" style="margin-right:10px"></i>Conference Reviewer
            </h3>
            
            <!-- 2025 -->
            <div style="margin:15px 0">
              <h4 style="color:#666;font-size:1.1em;margin-bottom:10px">2025</h4>
              <ul style="list-style:none;padding-left:20px">
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>International Conference on Learning Representations (<b>ICLR</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>Artificial Intelligence and Statistics (<b>AISTATS</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>Computer Vision and Pattern Recognition (<b>CVPR</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>International Conference on Machine Learning (<b>ICML</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>International Conference on Computer Vision (<b>ICCV</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</span>
                </li>
              </ul>
            </div>

            <!-- 2024 -->
            <div style="margin:15px 0">
              <h4 style="color:#666;font-size:1.1em;margin-bottom:10px">2024</h4>
              <ul style="list-style:none;padding-left:20px">
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>Computer Vision in the Wild (<b>CVinW</b>) Workshop at CVPR</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>European Conference on Computer Vision (<b>ECCV</b>)</span>
                </li>
                <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                  <span style="position:absolute;left:0;color:#2962ff">•</span>
                  <span>Conference on Neural Information Processing Systems (<b>NeurIPS</b>)</span>
                </li>
              </ul>
            </div>
          </div>

          <!-- Organizer Section -->
          <div style="margin:20px 0;padding:20px;background:linear-gradient(to right,#f8f9fa,white);border-radius:10px;box-shadow:0 2px 5px rgba(0,0,0,0.05)">
            <h3 style="color:#2962ff;font-size:1.2em;margin-bottom:15px;display:flex;align-items:center">
              <i class="fas fa-users" style="margin-right:10px"></i>Workshop Organizer
            </h3>
            <ul style="list-style:none;padding-left:20px">
              <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                <span style="position:absolute;left:0;color:#2962ff">•</span>
                <span>Computer Vision in the Wild (<b>CVinW</b>) Workshop at CVPR 2025</span>
              </li>
              <li style="margin:8px 0;padding-left:15px;position:relative;display:flex;align-items:center">
                <span style="position:absolute;left:0;color:#2962ff">•</span>
                <span>Computer Vision in the Wild (<b>CVinW</b>) Workshop at CVPR 2023</span>
              </li>
            </ul>
          </div>
        </td>
      </tr>
    </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:right;font-size:small;">
              Amazing template by <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks!
            </p>
          </td>
        </tr>
      </tbody></table>
    </td>
  </tr>
</table>

<tr>
  <td colspan="2" style="padding:0">
    <hr style="border:none;height:1px;background:linear-gradient(to right,transparent,#ddd,transparent);margin:20px 0">
  </td>
</tr>

<!-- 在body最后添加模态框结构 -->
<div id="imageModal" class="modal">
  <span class="close-modal">&times;</span>
  <img class="modal-content" id="modalImage">
</div>

<!-- 添加JavaScript代码，可放在body结束标签前 -->
<script>
  // 获取模态框元素
  var modal = document.getElementById("imageModal");
  var modalImg = document.getElementById("modalImage");
  
  // 为所有研究图片添加点击事件
  document.addEventListener('DOMContentLoaded', function() {
    var images = document.querySelectorAll('.research-image');
    images.forEach(function(img) {
      img.onclick = function() {
        modal.style.display = "block";
        modalImg.src = this.src;
      }
    });
    
    // 点击关闭按钮关闭模态框
    var closeBtn = document.getElementsByClassName("close-modal")[0];
    closeBtn.onclick = function() {
      modal.style.display = "none";
    }
    
    // 点击模态框外部区域也可关闭
    modal.onclick = function(event) {
      if (event.target == modal) {
        modal.style.display = "none";
      }
    }
  });
</script>

</body>
</html>
