<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Tianhe Ren</title>
  
  <meta name="author" content="Tianhe Ren">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tianhe Ren</name>
              </p>
              <p> 
                I am currently working at <a href="https://idea.edu.cn/">The International Digital Economy Academy (IDEA)</a> as Computer Vision Engineer, advised by Prof. <a href="https://www.leizhang.org/"> Lei Zhang </a>. In 2021, I got my bachelor's degree from <a href="https://mac.xmu.edu.cn/">MAC Lab</a> in Xiamen University advised by
                Associate Prof. <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao">Yiyi Zhou</a> and Prof. <a href="https://mac.xmu.edu.cn/rrji/">Rongrong Ji</a>. I'm primarily interested in researching <strong>vision foundation models, object detection and segmentation, and multi-modal learning</strong>. I'm also passionate about open-source projects in AI community.
                The research work and open-source projects I'm involved in have garnered almost <strong>20.0K stars</strong> on Github.
              </p>
              <p style="text-align:center">
                <a href="mailto:rentianhe@idea.edu.cn">Email</a> &nbsp/&nbsp
                <!-- <a href="files/CV_YongmingRao.pdf">CV</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/rentainhe"> Github </a>&nbsp/&nbsp
                <a href="https://www.zhihu.com/people/shi-zhi-tou-xi-de-yang-guang">ZhiHu</a> 
              </p>
            </td>
            <!-- <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:50%;max-width:50%" alt="profile photo" src="images/personal_avatar.jpg">
            </td> -->
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Research</heading></p>
              <p>
                See full list at <a href="https://scholar.google.com/citations?user=cW4ILs0AAAAJ&hl=zh-CN&oi=ao">Google Scholar.</a> (* indicates equal contribution, # indicates corresponding author). Representative papers or projects are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/grounded_sam_paper.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks</papertitle>
              </a>
              <br>
              <strong>Tianhe Ren</strong>,
              <a href="https://luogen1996.github.io/"> Shilong Liu</a>,
              <a href="https://ailingzeng.site/"> Ailing Zeng</a>,
              <a href="https://jinglin7.github.io/"> Jing Lin</a>,
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Kunchang Li</a>,
              <a href="https://scholar.google.com/citations?user=tLZ2V2kAAAAJ&hl=zh-CN"> He Cao</a>,
              <a href="https://github.com/tuofeilunhifi"> Jiayu Chen</a>,
              <a href="https://xinyu1205.github.io/"> Xinyu Huang</a>,
              <a href="https://yukangchen.com/"> Yukang Chen</a>,
              <a href="https://scholar.google.com/citations?user=gO4divAAAAAJ&hl=en&oi=sra"> Feng Yan</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN"> Zhaoyang Zeng</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <a href="https://yangjie-cv.github.io/"> Jie Yang</a>,
              <a href="https://scholar.google.com/citations?user=zdgHNmkAAAAJ&hl=zh-CN"> Hongyang Li</a>,
              <a href="https://mountchicken.github.io/"> Qing Jiang</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              Tech report, Jan. 2024
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2401.14159">arXiv</a> /
                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">code</a> /
                <a href="https://zhuanlan.zhihu.com/p/620271321">introduction (ZhiHu)</a>
              </div>
              <p>An overview technical report about our <a href="">Grounded-SAM</a> project, involving its base pipeline and more applications.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/llavag_arch.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models</papertitle>
              </a>
              <br>
              <a href="https://haozhang534.github.io/">Hao Zhang*</a>,
              <a href="https://scholar.google.com/citations?user=zdgHNmkAAAAJ&hl=zh-CN">Hongyang Li*</a>,
              <a href="https://fengli-ust.github.io/">Feng Li</a>,
              <strong>Tianhe Ren</strong>,
              <a href="https://maureenzou.github.io/">Xueyan Zou</a>,
              <a href="https://www.lsl.zone/">Shilong Liu</a>,
              <a href="https://sega-hsj.github.io/">Shijia Huang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao#</a>,
              <a href="https://www.leizhang.org/">Lei Zhang# </a>,
              <a href="https://chunyuan.li/">Chunyuan Li*</a>,
              <a href="https://jwyang.github.io/">Jianwei Yang*</a>
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              Tech report, Dec. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://llava-vl.github.io/llava-grounding/">project page</a> /
                <a href="https://llava-grounding.deepdataspace.com/">online demo</a> /
                <!-- <a href="https://www.youtube.com/watch?v=engIEhZogAQ">video</a> / -->
                <a href="https://arxiv.org/abs/2312.02949">arXiv</a> /
                <a href="https://github.com/UX-Decoder/LLaVA-Grounding">code</a>
              </div>
              <p>LLaVA-Grounding connects Large Multimodal Model (LMM) with a grounding model to facilitate grounded visual chat.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DINOv.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>Visual In-Context Prompting</papertitle>
              </a>
              <br>
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <a href="https://mountchicken.github.io/"> Qing Jiang</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
              <strong>Tianhe Ren</strong>,
              <a href="https://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://maureenzou.github.io/"> Xueyan Zou</a>,
              <a href="https://scholar.google.com/citations?user=zgaTShsAAAAJ&hl=zh-CN"> Huaizhe Xu</a>,
              <a href="https://scholar.google.com/citations?user=zdgHNmkAAAAJ&hl=zh-CN"> Hongyang Li</a>,
              <a href="https://chunyuan.li/"> Chunyuan Li</a>,
              <a href="https://jwyang.github.io/"> Jianwei Yang#</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/"> Jianfeng Gao#</a>,
              <br>
              Tech report, Nov. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <!-- <a href="https://llava-vl.github.io/llava-plus/">project page</a> /
                <a href="https://deepdataspace.com/playground/ivp">online demo</a> /
                <a href="https://www.youtube.com/watch?v=engIEhZogAQ">video</a> / -->
                <a href="https://arxiv.org/abs/2311.13601">arXiv</a> /
                <a href="https://github.com/UX-Decoder/DINOv">code</a>
              </div>
              <p>DINOv is a visual in-context prompting framework for referring and generic segmentation tasks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/trex-img.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>T-Rex: Counting by Visual Prompting</papertitle>
              </a>
              <br>
              <a href="https://mountchicken.github.io/"> Qing Jiang</a>,
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <strong>Tianhe Ren</strong>,
              <a href="https://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=U_cvvUwAAAAJ"> Zhaoyang Zeng</a>,
              Kent Yu,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>,
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              Tech report, Nov. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://llava-vl.github.io/llava-plus/">project page</a> /
                <a href="https://deepdataspace.com/playground/ivp">online demo</a> /
                <a href="https://www.youtube.com/watch?v=engIEhZogAQ">video</a> /
                <a href="https://arxiv.org/abs/2311.05437">arXiv</a> /
                <a href="https://github.com/IDEA-Research/T-Rex">code</a>
              </div>
              <p>T-Rex is an object counting model that can first detect then count any objects through visual prompting.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/llava-plus-hero.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>LLaVA-Plus: Large Language and Vision Assistants that Plug and Learn to Use Skills</papertitle>
              </a>
              <br>
              <a href="https://luogen1996.github.io/"> Shilong Liu</a>,
              <a href="https://sites.google.com/site/hcheng2site"> Hao Cheng</a>,
              <a href="https://hliu.cc/"> Haotian Liu</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
              <a href="https://fengli-ust.github.io/"> Feng Li</a>,
              <strong>Tianhe Ren</strong>,
              <a href="https://maureenzou.github.io/"> Xueyan Zou</a>,
              <a href="https://jwyang.github.io/"> Jianwei Yang</a>,
              <a href="https://www.suhangss.me/"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang</a>,
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/"> Jianfeng Gao</a>,
              <a href="https://chunyuan.li/"> Chunyuan Li#</a>
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              Tech report, Oct. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://llava-vl.github.io/llava-plus/">project page</a> /
                <a href="https://arxiv.org/abs/2311.05437">arXiv</a> /
                <a href="https://github.com/LLaVA-VL/LLaVA-Plus-Codebase">code</a>
              </div>
              <p>Extending <a href="https://llava-vl.github.io/">LLaVA</a> by incorporating a large and diverse set of external tools that can be selected, composed, and activated on the fly for performing tasks.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/lavin.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15023">
              <papertitle>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</papertitle>
              </a>
              <br>
              <a href="https://luogen1996.github.io/"> Gen Luo</a>,
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
              <strong>Tianhe Ren</strong>,
              Shengxin Chen,
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun</a>, 
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji#</a>
              <br>
              <!-- <em>NeurIPS</em>, 2023 -->
              Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2305.15023">arXiv</a> /
                <a href="https://github.com/luogen1996/LaVIN">code</a>
              </div>
              <p>A novel parameter-effective method for enhancing large language models' vision-language capabilities. When applied to LLaMA, our LaVIN demonstrates competitive performance in both single-modality and multi-modality tasks, with significant efficiency and reduced training costs.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/Grounded_SAM.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
              <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">
              <papertitle>Grounded-SAM: Detect and Segment Anything with Text Prompt</papertitle>
              </a>
              <br>
              <strong>Tianhe Ren*</strong>,
              <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
              <a href="https://scholar.google.com/citations?user=D4tLSbsAAAAJ&hl=zh-CN"> Kunchang Li</a>, 
              <a href="https://ailingzeng.site/"> Ailing Zeng</a>, 
              <a href="https://github.com/CiaoHe"> He Cao</a>, 
              <a href="https://github.com/tuofeilunhifi"> Jiayu Chen </a>, 
              <a href="https://jinglin7.github.io/"> Jing Lin</a>, 
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>, 
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,  
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>, 
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN">Zhaoyang Zeng</a>, 
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              <!-- <em>ICCV Demo Track</em>, 2023 &nbsp <font color="red"><strong>(Github Trending Top-1 Project)</strong></font> -->
              <!-- International Conference on Computer Vision (<strong>ICCV</strong>) Demo Track, 2023 &nbsp <font color="red"><strong>(Github Trending Top-1 Project)</strong></font> -->
              International Conference on Computer Vision (<strong>ICCV</strong>) Demo Track, 2023
              <br>
              <font color="red"><strong>(Github Trending Top-1 Project)</strong></font>
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://www.youtube.com/watch?v=oEQYStnF2l8">video</a> /
                <a href="https://github.com/IDEA-Research/Grounded-Segment-Anything">code</a> /
                <a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/automated-dataset-annotation-and-evaluation-with-grounding-dino-and-sam.ipynb">colab</a> /
                <a href="https://modelscope.cn/studios/tuofeilunhifi/Grounded-Segment-Anything/summary">modelscope</a> /
                <a href="https://zhuanlan.zhihu.com/p/620271321">introduction (ZhiHu)</a>
              </div>
              <p> A strong vision foundation model pipeline by combining Grounding-DINO and Segment-Anything-Model which can detect and segment everything with arbitrary text prompts.
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/DFA3D.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2307.12972">
              <papertitle>DFA3D: 3D Deformable Attention For 2D-to-3D Feature Lifting</papertitle>
              </a>
              <br>
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li*</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang*</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>, 
              <a href="http://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>, 
              <strong>Tianhe Ren</strong>, 
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              <!-- <em>ICCV</em>, 2023 -->
              International Conference on Computer Vision (<strong>ICCV</strong>), 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.04742">arXiv</a> /
                <a href="https://github.com/IDEA-Research/3D-deformable-attention">code</a>
              </div>
              <p>A new operator named 3D-Deformable-Attention for 2D to 3D feature lifting, which can be used and boost the performance in a range of 3D detection models.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/stable_dino_new.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2304.04742">
              <papertitle>Detection Transformer with Stable Matching</papertitle>
              </a>
              <br>
              <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
              <strong>Tianhe Ren*</strong>, 
              <a href="https://github.com/tuofeilunhifi"> Jiayu Chen*</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>,
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>,  
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
              <a href="https://github.com/IDEA-Research/Stable-DINO"> Jun Huang</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              <!-- <em>ICCV</em>, 2023 -->
              International Conference on Computer Vision (<strong>ICCV</strong>), 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2304.04742">arXiv</a> /
                <a href="https://github.com/IDEA-Research/Stable-DINO">code</a>
              </div>
              <p>Addressed the unstable matching issue in DETR-based models caused by multi-path optimization, by introducing a simple and efficient loss design that uses position metrics to supervise the classification scores of positive examples.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/detrex.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2306.07265">
              <papertitle>detrex: Benchmarking Detection Transformers</papertitle>
              </a>
              <br>
              <strong>Tianhe Ren*</strong>,
              <a href="http://www.lsl.zone/"> Shilong Liu*</a>,
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li*</a>,
              <a href="https://haozhang534.github.io/"> Hao Zhang*</a>,
              <a href="https://ailingzeng.site/"> Ailing Zeng</a>,
              <a href="https://github.com/yangjie-cv"> Jie Yang</a>,
              <a href="https://github.com/L1aoXingyu"> Xingyu Liao</a>,
              <a href="https://github.com/JiaDingCN"> Ding Jia</a>,
              <a href="https://github.com/LHY-HongyangLi"> Hongyang Li</a>,
              <a href="https://github.com/CiaoHe"> He Cao</a>,
              <a href="https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=en&oi=ao"> Jianan Wang</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN"> Zhaoyang Zeng</a>,
              <a href="https://scholar.google.com/citations?user=odjSydQAAAAJ&hl=zh-CN"> Xianbiao Qi</a>,
              <a href="https://scholar.google.com/citations?user=PzyvzksAAAAJ&hl=en"> Yuhui Yuan</a>,
              <a href="https://jwyang.github.io/"> Jianwei Yang</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              Tech report, May. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://rentainhe.github.io/projects/detrex/">project page</a> /
                <a href="https://arxiv.org/abs/2306.07265">arXiv</a> /
                <a href="https://github.com/IDEA-Research/detrex">code</a> /
                <a href="https://zhuanlan.zhihu.com/p/571307593">introduction (ZhiHu)</a>
              </div>
              <p>A standardized and unified benchmarking tool for Transformer-based object detection, segmentation, pose estimation and other visual recognition tasks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/GroundingDINO.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle" bgcolor="#ffffd0">
              <a href="https://arxiv.org/abs/2303.05499">
              <papertitle>Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection</papertitle>
              </a>
              <br>
              <a href="http://www.lsl.zone/"> Shilong Liu</a>,
              <a href="https://scholar.google.com/citations?user=U_cvvUwAAAAJ&hl=zh-CN&oi=ao">Zhaoyang Zeng</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=ybRe9GcAAAAJ&hl=zh-CN"> Feng Li</a>, 
              <a href="https://haozhang534.github.io/"> Hao Zhang</a>, 
              <a href="https://github.com/yangjie-cv"> Jie Yang</a>,
              <a href="https://scholar.google.com/citations?user=Zd7WmXUAAAAJ&hl=zh-CN&oi=ao"> Chunyuan Li</a>,
              <a href="https://jwyang.github.io/"> Jianwei Yang</a>,
              <a href="https://scholar.google.com/citations?user=dxN1_X0AAAAJ&hl=zh-CN&oi=ao"> Hang Su</a>,
              <a href="https://ml.cs.tsinghua.edu.cn/~jun/index.shtml"> Jun Zhu</a>,
              <a href="https://www.leizhang.org/"> Lei Zhang#</a>
              <br>
              <!-- <em>Tech Report</em>, 2023 -->
              Tech report, Apr. 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://www.youtube.com/watch?v=wxWDt5UiwY8">video</a> /
                <a href="https://arxiv.org/abs/2303.05499">arXiv</a> /
                <a href="https://github.com/IDEA-Research/GroundingDINO">code</a> /
                <a href="https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/zero-shot-object-detection-with-grounding-dino.ipynb">colab</a>
              </div>
              <p>A simple and strong DETR-based framework for open-set detection, achieving zero-shot 52.5 AP on COCO (training without COCO data).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/CVPR23_YOSO.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.14651">
              <papertitle>You Only Segment Once: Towards Real-Time Panoptic Segmentation</papertitle>
              </a>
              <br>
              <a href="https://hujiecpp.github.io/"> Jie Hu</a>,
              <a> Linyan Huang</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=GToqXScAAAAJ&hl=zh-CN&oi=ao"> Shengchuan Zhang</a>, 
              <a href="https://scholar.google.com/citations?user=lRSD7PQAAAAJ&hl=zh-CN"> Rongrong Ji</a>, 
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN&oi=ao"> Liujuan Cao#</a>,
              <br>
              <!-- <em>CVPR</em>, 2023 -->
              Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2023
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2303.14651">arXiv</a> /
                <a href="https://github.com/hujiecpp/YOSO">code</a>
              </div>
              <p>A novel framework for real-time panoptic segmentation task with competitive performance compared to state-of-the-art methods.</p>
              <!-- <a href="https://arxiv.org/abs/2303.14651">[arXiv]</a>
              <a href="https://github.com/hujiecpp/YOSO">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/hujiecpp/YOSO?style=social"> -->
              <!-- <p> YOSO designed a <b>light-weight feature pyramid aggregator</b> and a <b>seperable dynamic convolution attention module</b> to speed up the segmentation pipeline.
                To the best of our knowledge, YOSO is <b>the first real-time panoptic segmentation framework</b> with competitive performance compared to state-of-the-art methods.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/iu_vit.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2212.13771">
              <papertitle>Exploring Vision Transformers as Diffusion Learners</papertitle>
              </a>
              <br>
              <a href="https://github.com/CiaoHe"> He Cao</a>,
              <a href="https://scholar.google.com/citations?user=mt5mvZ8AAAAJ&hl=zh-CN&oi=ao">Jianan Wang</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=odjSydQAAAAJ&hl=zh-CN&oi=ao"> Xianbiao Qi</a>, 
              <a href="https://scholar.google.com/citations?user=IA_3Jq8AAAAJ&hl=zh-CN&oi=ao"> Yihao Chen </a>, 
              <a href="https://yao-lab.github.io/"> Yuan Yao# </a>,
              <a href="https://www.leizhang.org/"> Lei Zhang# </a>
              <br>
              <!-- <em>Tech Report</em>, 2022 -->
              Tech report, Oct. 2022
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2212.13771">arXiv</a>
              </div>
              <p>A plain, non-hierarchical Vision Transformer (ViT) backbone for diffusion models.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/SparseSAM.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2210.05177">
              <papertitle>Make Sharpness-Aware Minimization Stronger: A Sparsified Perturbation Approach</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=PTM4HCsAAAAJ&hl=zh-CN&oi=ao"> Peng Mi</a>,
              <a href="https://sites.google.com/site/mathshenli/home">Li Shen</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>, 
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun</a>, 
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji#</a>,
              <a href="https://www.sydney.edu.au/engineering/about/our-people/academic-staff/dacheng-tao.html"> Dacheng Tao# </a>
              <br>
              <!-- <em>NeurIPS</em>, 2022 -->
              Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>), 2022
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://arxiv.org/abs/2210.05177">arXiv</a> /
                <a href="https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization">code</a>
              </div>
              <p>An efficient variant of SAM optimizer achieved by computing a sparse perturbation based on fisher information and dynamic sparse training.</p>
              <!-- <a href="https://arxiv.org/abs/2210.05177">[arXiv]</a>
              <a href="https://github.com/Mi-Peng/Sparse-Sharpness-Aware-Minimization">[Code]</a> -->
              <!-- <img alt="Github stars" src="https://img.shields.io/github/stars/Mi-Peng/Sparse-Sharpness-Aware-Minimization?style=social"> -->
              <!-- <p> SparseSAM is an efficient variant of SAM optimizer achieved by computing a <b>sparse perturbation</b> based on fisher information and dynamic sparse training.</p> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/TRAR.png" alt="dise">
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">
              <papertitle>TRAR: Routing the Attention Spans in Transformers for Visual Question Answering</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.com/citations?user=w3_2ep0AAAAJ&hl=zh-CN&oi=ao"> Yiyi Zhou</a>,
              <strong>Tianhe Ren</strong>, 
              <a href="https://scholar.google.com.hk/citations?user=jkxdiToAAAAJ&hl=zh-CN&oi=ao"> Chaoyang Zhu </a>, 
              <a href="https://sites.google.com/view/xssun"> Xiaoshuai Sun#</a>, 
              <a href="https://people.ucas.ac.cn/~jzliu?language=en"> Jianzhuang Liu</a>,
              <a href="https://scholar.google.com/citations?user=k5hVBfMAAAAJ&hl=zh-CN"> Xinghao Ding</a>,
              <a href="https://scholar.google.com.hk/citations?user=u-8x34cAAAAJ&hl=zh-CN"> Mingliang Xu</a>,
              <a href="https://mac.xmu.edu.cn/rrji/"> Rongrong Ji#</a>
              <br>
              International Conference on Computer Vision (<strong>ICCV</strong>), 2021
              <br>
              <div class="paper" id="boundary_iou">
                <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_TRAR_Routing_the_Attention_Spans_in_Transformer_for_Visual_Question_ICCV_2021_paper.pdf">arXiv</a> /
                <a href="https://github.com/rentainhe/TRAR-VQA">code</a>
              </div>
              <p>A novel dynamic routing attention mechanism brings a consistent performance gain for a range of vision and language tasks.</p>
            </td>
          </tr>

        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Amazing template by <a href="https://jonbarron.info/">Jon Barron</a>. Big thanks!
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

</html>
